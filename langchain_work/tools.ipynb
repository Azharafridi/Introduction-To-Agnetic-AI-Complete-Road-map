{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "398e646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tool decorator\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def division(a:int, b:int) -> int:\n",
    "    \"\"\"divid two numbers\"\"\"\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a69ad6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "division\n",
      "divid two numbers\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "# we can print from it\n",
    "print(division.name)\n",
    "print(division.description)\n",
    "print(division.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "69b0b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "## async implementation\n",
    "\n",
    "@tool\n",
    "async def mulitplication(a:int, b:int) -> int:\n",
    "    \"\"\"mulitplication of two numbers\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9202beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "\n",
    "@tool\n",
    "def multiply_by_max(\n",
    "    a:Annotated[int,\"A value\"],\n",
    "    b:Annotated[List[int], \"list of ints over which to take maximum\"]\n",
    ")-> int:\n",
    "    \"\"\"Multiply a by the max of b\"\"\"\n",
    "    return a * max(b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "095d3e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': {'description': 'A value', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'list of ints over which to take maximum', 'items': {'type': 'integer'}, 'title': 'B', 'type': 'array'}}\n"
     ]
    }
   ],
   "source": [
    "print(multiply_by_max.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49fc1c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "## structured tool\n",
    "\n",
    "from langchain_core.tools import StructuredTool\n",
    "import asyncio\n",
    "def mulitplication(a:int, b:int) -> int:\n",
    "    \"\"\"mulitplication of two numbers\"\"\"\n",
    "    return a * b\n",
    "\n",
    "async def amulitplication(a:int, b:int) -> int:\n",
    "    \"\"\"mulitplication of two numbers\"\"\"\n",
    "    return a * b\n",
    "\n",
    "calculator = StructuredTool.from_function(func=mulitplication, coroutine=amulitplication)\n",
    "\n",
    "print(calculator.invoke({\"a\":2, \"b\":3}))\n",
    "print(await calculator.ainvoke({\"a\":2, \"b\":5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "342cb6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis a\n"
     ]
    }
   ],
   "source": [
    "## inbuilt tools\n",
    "\n",
    "# wikipedia inegration\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results = 5, doc_content_chars_max = 300)\n",
    "tool = WikipediaQueryRun(api_wrapper = api_wrapper)\n",
    "\n",
    "print(tool.invoke({\"query\": \"langchain\"}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7a22d013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3b0904e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "36ff0b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "tool = TavilySearch(\n",
    "    max_results = 5, \n",
    "    topic = \"general\",\n",
    "    #include_answer = False,\n",
    "    #include_raw_content = False,\n",
    "    #include_images = False,\n",
    "    #include_image_descriptions = False,\n",
    "    #search_depth = \"basic\",messages = []\n",
    "    #time_range = \"day\",\n",
    "    #include_domains = None,\n",
    "    #exclude_domains = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25a7b318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what is the recent development in AI?',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'url': 'https://www.crescendo.ai/news/latest-ai-news-and-updates',\n",
       "   'title': 'The Latest AI News and AI Breakthroughs that Matter Most: 2025',\n",
       "   'content': 'Summary: MIT researchers have developed a generative AI system named FlowER (Flow matching for Electron Redistribution) that predicts chemical reactions while',\n",
       "   'score': 0.67321366,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://ai.koombea.com/blog/key-ai-developments',\n",
       "   'title': '8 Key AI Developments Shaping 2025 - Koombea',\n",
       "   'content': 'The developments highlighted here – generative AI, AI copilots, coding AI, healthcare breakthroughs, AI in security, regulation, and robotics –',\n",
       "   'score': 0.5518883,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://ep.jhu.edu/news/advancements-in-ai-and-machine-learning/',\n",
       "   'title': 'Advancements in AI and Machine Learning',\n",
       "   'content': 'Advances in ML, natural language processing (NLP), and computer vision have enabled AI to perform ever-more complex tasks such as diagnosing',\n",
       "   'score': 0.49670476,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.artificialintelligence-news.com/',\n",
       "   'title': 'AI News | Latest AI News, Analysis & Events',\n",
       "   'content': \"Thailand becomes one of the first in Asia to get the Sora app · Malaysia launches Ryt Bank, its first AI-powered bank · Google's Veo 3 AI video creation tools are\",\n",
       "   'score': 0.43640643,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://news.microsoft.com/source/features/ai/6-ai-trends-youll-see-more-of-in-2025/',\n",
       "   'title': \"6 AI trends you'll see more of in 2025 - Microsoft Source\",\n",
       "   'content': 'One of the biggest developments this coming year can be summarized in two words: testing and customization. If you can measure risks and threats',\n",
       "   'score': 0.43208793,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 1.01,\n",
       " 'request_id': '5c0ac64d-e2f8-43dc-ba59-6ec02e235a92'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.invoke(\"what is the recent development in AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bba5a9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Published: 2023-08-02\\nTitle: Attention Is All You Need\\nAuthors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\\nSummary: The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "\n",
    "tool = ArxivAPIWrapper()\n",
    "tool.run(\"1706.03762\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ec16e7",
   "metadata": {},
   "source": [
    "### call Tool with LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "882299c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis a\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results = 5, doc_content_chars_max = 300)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper = api_wrapper)\n",
    "\n",
    "print(wiki_tool.invoke({\"query\": \"langchain\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e20a22ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "tavily_tool = TavilySearch(\n",
    "    max_results = 5, \n",
    "    topic = \"general\",\n",
    "    #include_answer = False,\n",
    "    #include_raw_content = False,\n",
    "    #include_images = False,\n",
    "    #include_image_descriptions = False,\n",
    "    #search_depth = \"basic\",\n",
    "    #time_range = \"day\",\n",
    "    #include_domains = None,\n",
    "    #exclude_domains = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "582ba88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "@tool\n",
    "def add(a:int, b:int) -> int:\n",
    "    \"\"\"add  two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a:int, b:int) -> int:\n",
    "    \"\"\"mulitplication of two numbers\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1d8185da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [wiki_tool,add, multiply, tavily_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "36f455e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/home/azhar/Desktop/Azhar/BlueScarf/AgenticAI/Road_map/Introduction-To-Agnetic-AI-Complete-Road-map/venv/lib/python3.10/site-packages/wikipedia/__init__.py'>, top_k_results=5, lang='en', load_all_available_meta=False, doc_content_chars_max=300)),\n",
       " StructuredTool(name='add', description='add  two numbers', args_schema=<class 'langchain_core.utils.pydantic.add'>, func=<function add at 0x79936af69000>),\n",
       " StructuredTool(name='multiply', description='mulitplication of two numbers', args_schema=<class 'langchain_core.utils.pydantic.multiply'>, func=<function multiply at 0x79936af6a200>),\n",
       " TavilySearch(max_results=5, topic='general', api_wrapper=TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********'), api_base_url=None))]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07b6d229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x799369607280>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x79936a5c8100>, model_name='qwen/qwen3-32b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"qwen/qwen3-32b\", model_provider = \"groq\")\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4311b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x799369607280>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x79936a5c8100>, model_name='qwen/qwen3-32b', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'add', 'description': 'add  two numbers', 'parameters': {'properties': {'a': {'type': 'integer'}, 'b': {'type': 'integer'}}, 'required': ['a', 'b'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'multiply', 'description': 'mulitplication of two numbers', 'parameters': {'properties': {'a': {'type': 'integer'}, 'b': {'type': 'integer'}}, 'required': ['a', 'b'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'tavily_search', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. It not only retrieves URLs and snippets, but offers advanced search depths, domain management, time range filters, and image search, this tool delivers real-time, accurate, and citation-backed results.Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'Search query to look up', 'type': 'string'}, 'include_domains': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'}, {'type': 'null'}], 'default': [], 'description': 'A list of domains to restrict search results to.\\n\\n        Use this parameter when:\\n        1. The user explicitly requests information from specific websites (e.g., \"Find climate data from nasa.gov\")\\n        2. The user mentions an organization or company without specifying the domain (e.g., \"Find information about iPhones from Apple\")\\n\\n        In both cases, you should determine the appropriate domains (e.g., [\"nasa.gov\"] or [\"apple.com\"]) and set this parameter.\\n\\n        Results will ONLY come from the specified domains - no other sources will be included.\\n        Default is None (no domain restriction).\\n        '}, 'exclude_domains': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'}, {'type': 'null'}], 'default': [], 'description': 'A list of domains to exclude from search results.\\n\\n        Use this parameter when:\\n        1. The user explicitly requests to avoid certain websites (e.g., \"Find information about climate change but not from twitter.com\")\\n        2. The user mentions not wanting results from specific organizations without naming the domain (e.g., \"Find phone reviews but nothing from Apple\")\\n\\n        In both cases, you should determine the appropriate domains to exclude (e.g., [\"twitter.com\"] or [\"apple.com\"]) and set this parameter.\\n\\n        Results will filter out all content from the specified domains.\\n        Default is None (no domain exclusion).\\n        '}, 'search_depth': {'anyOf': [{'enum': ['basic', 'advanced'], 'type': 'string'}, {'type': 'null'}], 'default': 'basic', 'description': 'Controls search thoroughness and result comprehensiveness.\\n    \\n        Use \"basic\" for simple queries requiring quick, straightforward answers.\\n        \\n        Use \"advanced\" (default) for complex queries, specialized topics, \\n        rare information, or when in-depth analysis is needed.\\n        '}, 'include_images': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}], 'default': False, 'description': 'Determines if the search returns relevant images along with text results.\\n   \\n        Set to True when the user explicitly requests visuals or when images would \\n        significantly enhance understanding (e.g., \"Show me what black holes look like,\" \\n        \"Find pictures of Renaissance art\").\\n        \\n        Leave as False (default) for most informational queries where text is sufficient.\\n        '}, 'time_range': {'anyOf': [{'enum': ['day', 'week', 'month', 'year'], 'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Limits results to content published within a specific timeframe.\\n        \\n        ONLY set this when the user explicitly mentions a time period \\n        (e.g., \"latest AI news,\" \"articles from last week\").\\n        \\n        For less popular or niche topics, use broader time ranges \\n        (\"month\" or \"year\") to ensure sufficient relevant results.\\n   \\n        Options: \"day\" (24h), \"week\" (7d), \"month\" (30d), \"year\" (365d).\\n        \\n        Default is None.\\n        '}, 'topic': {'anyOf': [{'enum': ['general', 'news', 'finance'], 'type': 'string'}, {'type': 'null'}], 'default': 'general', 'description': 'Specifies search category for optimized results.\\n   \\n        Use \"general\" (default) for most queries, INCLUDING those with terms like \\n        \"latest,\" \"newest,\" or \"recent\" when referring to general information.\\n\\n        Use \"finance\" for markets, investments, economic data, or financial news.\\n\\n        Use \"news\" ONLY for politics, sports, or major current events covered by \\n        mainstream media - NOT simply because a query asks for \"new\" information.\\n        '}, 'include_favicon': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}], 'default': False, 'description': \"Determines whether to include favicon URLs for each search result.\\n        \\n        When enabled, each search result will include the website's favicon URL,\\n        which can be useful for:\\n        - Building rich UI interfaces with visual website indicators\\n        - Providing visual cues about the source's credibility or brand\\n        - Creating bookmark-like displays with recognizable site icons\\n        \\n        Set to True when creating user interfaces that benefit from visual branding\\n        or when favicon information enhances the user experience.\\n        \\n        Default is False to minimize response size and API usage.\\n        \"}, 'start_date': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Filters search results to include only content published on or after this date.\\n        \\n        Use this parameter when you need to:\\n        - Find recent developments or updates on a topic\\n        - Exclude outdated information from search results\\n        - Focus on content within a specific timeframe\\n        - Combine with end_date to create a custom date range\\n        \\n        Format must be YYYY-MM-DD (e.g., \"2024-01-15\" for January 15, 2024).\\n        \\n        Examples:\\n        - \"2024-01-01\" - Results from January 1, 2024 onwards\\n        - \"2023-12-25\" - Results from December 25, 2023 onwards\\n        \\n        When combined with end_date, creates a precise date range filter.\\n        \\n        Default is None (no start date restriction).\\n        '}, 'end_date': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Filters search results to include only content published on or before this date.\\n        \\n        Use this parameter when you need to:\\n        - Exclude content published after a certain date\\n        - Study historical information or past events\\n        - Research how topics were covered during specific time periods\\n        - Combine with start_date to create a custom date range\\n        \\n        Format must be YYYY-MM-DD (e.g., \"2024-03-31\" for March 31, 2024).\\n        \\n        Examples:\\n        - \"2024-03-31\" - Results up to and including March 31, 2024\\n        - \"2023-12-31\" - Results up to and including December 31, 2023\\n        \\n        When combined with start_date, creates a precise date range filter.\\n        For example: start_date=\"2024-01-01\", end_date=\"2024-03-31\" \\n        returns results from Q1 2024 only.\\n        \\n        Default is None (no end date restriction).\\n        '}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools = llm.bind_tools(tools=tools)\n",
    "llm_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "efbc7a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'reasoning_content': 'Okay, the user is asking \"what is 2 * 3\". Let me see which tool to use here. The available functions are wikipedia, add, multiply, and tavily_search.\\n\\nThe question is a simple multiplication problem. The multiply function is designed for exactly this kind of task. The parameters required are two integers, a and b. Here, a is 2 and b is 3. \\n\\nI should check if there\\'s any reason not to use the multiply function. The user isn\\'t asking for a general explanation or historical context, so wikipedia isn\\'t needed. The add function is for addition, so that\\'s not applicable here. Tavily_search is for web searches, which would be overkill for a basic math problem.\\n\\nTherefore, the correct tool to use is multiply with arguments a=2 and b=3. That should give the user the answer they need efficiently.\\n', 'tool_calls': [{'id': 'agwwpekse', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 210, 'prompt_tokens': 2060, 'total_tokens': 2270, 'completion_time': 0.437982677, 'prompt_time': 0.09137178, 'queue_time': 0.424976435, 'total_time': 0.529354457, 'completion_tokens_details': {'reasoning_tokens': 181}}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_efa9879028', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--281da2b8-b4ff-45ec-b4ca-d86f87d694e3-0' tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'agwwpekse', 'type': 'tool_call'}] usage_metadata={'input_tokens': 2060, 'output_tokens': 210, 'total_tokens': 2270}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "query = \"what is 2 * 3\"\n",
    "messages = [HumanMessage(query)]\n",
    "response = llm_with_tools.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7467807c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 2, 'b': 3},\n",
       "  'id': 'agwwpekse',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a9f05fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'agwwpekse', 'type': 'tool_call'}\n"
     ]
    }
   ],
   "source": [
    "for tool_call in response.tool_calls:\n",
    "    print(tool_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "52523520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='what is 2 * 3', additional_kwargs={}, response_metadata={}),\n",
       " ToolMessage(content='6', name='mulitply', tool_call_id='agwwpekse')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "for tool_call in response.tool_calls:\n",
    "    selected_tool = {\"Add\": add, \"multiply\":mulitply, \"wikipedia\": wiki_tool,\"Tavily_Search\":tavily_tool} [tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "99eccf90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The result of multiplying 2 by 3 is 6. \\n\\n<answer>\\n6\\n</answer>', additional_kwargs={'reasoning_content': 'Okay, let\\'s see. The user asked \"what is 2 * 3\". That\\'s a multiplication question. I need to figure out which tool to use here.\\n\\nLooking at the available functions, there\\'s one called multiply. The description says it\\'s for multiplying two numbers. Perfect. The parameters are a and b, both integers. So I should call multiply with a=2 and b=3.\\n\\nWait, is there any chance they might have meant something else? Like, maybe a date or a different operation? But the * symbol clearly indicates multiplication here. No need to use the calculator tool since there\\'s a direct function for it.\\n\\nJust make sure the inputs are integers. 2 and 3 are both integers, so that\\'s good. The function should return 6. Yep, that\\'s straightforward. No need for any other tools here.\\n'}, response_metadata={'token_usage': {'completion_tokens': 201, 'prompt_tokens': 2070, 'total_tokens': 2271, 'completion_time': 0.400831141, 'prompt_time': 0.08973756, 'queue_time': 0.422003458, 'total_time': 0.490568701, 'completion_tokens_details': {'reasoning_tokens': 174}}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_fbaaebd3ce', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--8401818f-22a6-4c3b-b323-19e6738e2c0a-0', usage_metadata={'input_tokens': 2070, 'output_tokens': 201, 'total_tokens': 2271})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## now give this whole response to llm \n",
    "llm_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2d8bda97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, let\\'s see. The user is asking two things here: what LangChain is and the result of 5 multiplied by 15. I need to handle both parts.\\n\\nFirst, for LangChain. I remember that LangChain is a framework related to building applications with language models. But to give an accurate answer, I should probably look it up using the Wikipedia tool. Let me check the functions available. Oh, there\\'s a Wikipedia function that takes a query. So I\\'ll use that with \"LangChain\" as the query.\\n\\nNext, the math part: 5 * 15. That\\'s straightforward multiplication. The user probably expects a quick answer here. Since there\\'s a multiply function in the tools, I can use that. The parameters are a and b, both integers. So a=5 and b=15. Alternatively, I could calculate it directly, but using the provided function ensures consistency.\\n\\nWait, the tools include add and multiply functions. Let me confirm. Yes, multiply is there. So I\\'ll call multiply with a=5 and b=15. That should give 75. \\n\\nSo, I need to make two tool calls: one to Wikipedia for LangChain and another to multiply 5 and 15. The user might want both answers presented clearly. I should structure the response with both results once the tool calls are made.\\n', 'tool_calls': [{'id': 's4h5c3yk7', 'function': {'arguments': '{\"query\":\"LangChain\"}', 'name': 'wikipedia'}, 'type': 'function'}, {'id': 't5w5s1edf', 'function': {'arguments': '{\"a\":5,\"b\":15}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 333, 'prompt_tokens': 2067, 'total_tokens': 2400, 'completion_time': 0.691236889, 'prompt_time': 0.091316467, 'queue_time': 0.409338769, 'total_time': 0.782553356, 'completion_tokens_details': {'reasoning_tokens': 282}}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_78ac7f3229', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--28d7489e-be6e-4925-8248-d16e16c56e50-0', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'LangChain'}, 'id': 's4h5c3yk7', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 5, 'b': 15}, 'id': 't5w5s1edf', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2067, 'output_tokens': 333, 'total_tokens': 2400})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query = \"what is langchain and what is 5 * 15?\"\n",
    "messages = [HumanMessage(query)]\n",
    "ai_msg = llm_with_tools.invoke(query)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bd04f9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'wikipedia',\n",
       "  'args': {'query': 'LangChain'},\n",
       "  'id': 's4h5c3yk7',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'multiply',\n",
       "  'args': {'a': 5, 'b': 15},\n",
       "  'id': 't5w5s1edf',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bcd57686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='what is langchain and what is 5 * 15?', additional_kwargs={}, response_metadata={}),\n",
       " ToolMessage(content=\"Page: LangChain\\nSummary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis a\", name='wikipedia', tool_call_id='s4h5c3yk7'),\n",
       " ToolMessage(content='75', name='mulitply', tool_call_id='t5w5s1edf')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"Add\": add, \"multiply\":mulitply, \"wikipedia\": wiki_tool,\"Tavily_Search\":tavily_tool} [tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "064bc948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a software framework designed to integrate large language models (LLMs) into applications. It facilitates tasks like document analysis, enabling developers to build systems that leverage LLM capabilities for processing and generating human-like text. The framework supports workflows where LLMs are combined with other tools, data sources, or custom logic to create sophisticated AI-driven applications.\n",
      "\n",
      "The result of 5 multiplied by 15 is **75**.\n"
     ]
    }
   ],
   "source": [
    "#summerize it \n",
    "response = llm_with_tools.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c5a13c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
